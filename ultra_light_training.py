import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\nimport glob\nimport random\nimport numpy as np\n\n# Ultra-lightweight model for minimal VRAM usage\nclass UltraLightILWTModel(nn.Module):\n    \"\"\"\n    Ultra-lightweight model for minimal VRAM usage with 128x128 images\n    \"\"\"\n    def __init__(self, channels=6):\n        super(UltraLightILWTModel, self).__init__()\n        self.channels = channels\n        \n        # Ultra-compact convolution layers to minimize memory\n        # Using very small hidden channels\n        self.encoder = nn.Conv2d(channels, 8, kernel_size=3, padding=1)  # Reduced from 24 to 8\n        self.decoder = nn.Conv2d(8, channels, kernel_size=3, padding=1)\n        \n        # Initialize with identity to preserve information initially\n        nn.init.eye_(self.encoder.weight[:, :channels, 1, 1])  # Initialize center to identity\n        \n    def forward(self, x):\n        # Encode with minimal transformation\n        encoded = F.relu(self.encoder(x))\n        # Decode back to original channels\n        decoded = self.decoder(encoded)\n        return decoded, 0  # Return dummy log_det\n    \n    def inverse(self, z):\n        # Simple return as inverse (this is a simplified model)\n        return z\n\nclass UltraLightDataset(Dataset):\n    \"\"\"\n    Ultra-lightweight dataset for minimal memory usage\n    \"\"\"\n    def __init__(self, image_dir, img_size=128, transform=None):\n        png_files = glob.glob(os.path.join(image_dir, \"*.png\"))\n        jpg_files = glob.glob(os.path.join(image_dir, \"*.jpg\"))\n        jpeg_files = glob.glob(os.path.join(image_dir, \"*.jpeg\"))\n        self.image_paths = png_files + jpg_files + jpeg_files\n        self.img_size = img_size\n        \n        if transform is None:\n            self.transform = transforms.Compose([\n                transforms.Resize((img_size, img_size)),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n            ])\n        else:\n            self.transform = transform\n    \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        # Load host image\n        host_path = self.image_paths[idx]\n        host_img = Image.open(host_path).convert('RGB')\n        host_tensor = self.transform(host_img)\n        \n        # Select secret image\n        secret_idx = random.choice([i for i in range(len(self.image_paths)) if i != idx])\n        secret_path = self.image_paths[secret_idx]\n        secret_img = Image.open(secret_path).convert('RGB')\n        secret_tensor = self.transform(secret_img)\n        \n        # For ultra-lightweight approach, just return the tensors separately\n        # since complex steganography requires more memory\n        return torch.cat([host_tensor, secret_tensor], dim=0), host_tensor, secret_tensor\n\ndef ultra_light_train(model, dataset, num_epochs=2, max_batches=10):  # Very limited training\n    \"\"\"\n    Ultra-lightweight training that minimizes VRAM usage\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Ultra-light training on {device}\")\n    \n    model = model.to(device)\n    \n    # Minimal parameters\n    learning_rate = 5e-5  # Very small learning rate\n    batch_size = 1\n    \n    # Ultra-conservative optimizer\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)  # SGD uses less memory than Adam\n    \n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    print(f\"Starting ultra-light training for {num_epochs} epochs, max {max_batches} batches...\")\n    \n    model.train()\n    \n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        \n        batch_count = 0\n        for batch_idx, (input_tensor, host_tensor, secret_tensor) in enumerate(dataloader):\n            if batch_count >= max_batches:\n                break\n                \n            # Move to device\n            input_tensor = input_tensor.to(device)\n            host_tensor = host_tensor.to(device)\n            secret_tensor = secret_tensor.to(device)\n            \n            # Forward pass\n            output, _ = model(input_tensor)\n            \n            # Very simple loss for ultra-light training\n            loss = F.mse_loss(output, input_tensor)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # Ultra-conservative gradient clipping\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n            \n            # Update parameters\n            optimizer.step()\n            \n            print(f\"  Batch {batch_idx+1}, Loss: {loss.item():.6f}\")\n            \n            batch_count += 1\n            \n            # Cleanup to minimize memory\n            del loss, output\n            if batch_idx % 5 == 0:  # Clear cache every few batches\n                torch.cuda.empty_cache()\n    \n    print(\"Ultra-light training completed!\")\n\ndef check_vram_usage():\n    \"\"\"\n    Check current VRAM usage\n    \"\"\"\n    if torch.cuda.is_available():\n        allocated = torch.cuda.memory_allocated() / 1024**2\n        reserved = torch.cuda.memory_reserved() / 1024**2\n        print(f\"Current VRAM - Allocated: {allocated:.2f} MB, Reserved: {reserved:.2f} MB\")\n        return allocated, reserved\n    else:\n        print(\"CUDA not available\")\n        return 0, 0\n\ndef main():\n    print(\"Ultra-Lightweight VRAM Minimization Test\")\n    print(\"=\" * 50)\n    \n    # Check initial VRAM\n    print(\"Initial VRAM usage:\")\n    check_vram_usage()\n    \n    # Create ultra-light model\n    print(\"\\nCreating ultra-light model...\")\n    model = UltraLightILWTModel(channels=6)\n    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n    \n    # Check VRAM after model creation\n    print(\"\\nVRAM after model creation:\")\n    check_vram_usage()\n    \n    # Load dataset with small size\n    print(\"\\nLoading dataset...\")\n    dataset = UltraLightDataset(\"my_images\", img_size=128)\n    print(f\"Dataset size: {len(dataset)} images\")\n    \n    # Check VRAM after dataset loading\n    print(\"\\nVRAM after dataset setup:\")\n    check_vram_usage()\n    \n    # Run ultra-light training\n    print(\"\\nBeginning ultra-light training...\")\n    ultra_light_train(model, dataset, num_epochs=1, max_batches=5)\n    \n    # Final VRAM check\n    print(\"\\nFinal VRAM usage:\")\n    check_vram_usage()\n    \n    # Save model\n    torch.save(model.state_dict(), 'ultra_light_model.pth')\n    print(f\"\\nUltra-light model saved as 'ultra_light_model.pth'\")\n\nif __name__ == \"__main__\":\n    main()\n